{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the problem\n",
    "Even though error rates are low, creating transition matrices from predicted labels gives very different results from the same matrices created from ground truth labels.\n",
    "\n",
    "Why?\n",
    "\n",
    "`vak.core.predict` currently does not use the same function that `vak.core.learncurve.test` uses to find segments from predicted timebin labels. The `vak.core.predict` function is more computationally expensive because it finds times of onsets and offsets, while the `vak.core.learncurve.test` function just finds wherever labels change and returning the first label after each change point (which will be the same for the rest of the segment).\n",
    "\n",
    "So worst case scenario would be if those functions give different results.\n",
    "There are tests for this already but maybe they are missing something that only emerges from bigger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load a network and get predictions\n",
    "you can ignore most of this code and scroll to comments below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "from glob import glob\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "import vak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VDS_PATH = Path(\n",
    "    '/home/nickledave/Documents/data/BFSongRepository/vak/gy6or6/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vds_path = str(VDS_PATH.joinpath('_prep_190726_153000.train.vds.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.5s\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "train_vds = vak.Dataset.load(json_fname=train_vds_path)\n",
    "\n",
    "if train_vds.are_spects_loaded() is False:\n",
    "    train_vds = train_vds.load_spects()\n",
    "\n",
    "X_train = train_vds.spects_list()\n",
    "X_train = np.concatenate(X_train, axis=1)\n",
    "Y_train = train_vds.lbl_tb_list()\n",
    "Y_train = np.concatenate(Y_train)\n",
    "# transpose so rows are time bins\n",
    "X_train = X_train.T\n",
    "\n",
    "n_classes = len(train_vds.labelmap)\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEETYNET_VDS_PATH = Path('/home/nickledave/Documents/repos/tweetynet/data/BFSongRepository/gy6or6/vds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vds_path = list(TWEETYNET_VDS_PATH.glob('*test.vds.json'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_replicates = 4\n",
    "train_set_durs = [60, 120, 480]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  6.1s\n"
     ]
    }
   ],
   "source": [
    "test_vds = vak.Dataset.load(json_fname=test_vds_path)\n",
    "\n",
    "if test_vds.are_spects_loaded() is False:\n",
    "    test_vds = test_vds.load_spects()\n",
    "\n",
    "if test_vds.labelmap != train_vds.labelmap:\n",
    "    raise ValueError(\n",
    "        f'labelmap of test set, {test_vds.labelmap}, does not match labelmap of training set, '\n",
    "        f'{train_vds.labelmap}'\n",
    "    )\n",
    "\n",
    "def unpack_test():\n",
    "    \"\"\"helper function because we want to get back test set unmodified every time we go through\n",
    "    main loop below, without copying giant arrays\"\"\"\n",
    "    X_test = test_vds.spects_list()\n",
    "    X_test = np.concatenate(X_test, axis=1)\n",
    "    # transpose so rows are time bins\n",
    "    X_test = X_test.T\n",
    "    Y_test = test_vds.lbl_tb_list()\n",
    "    Y_test = np.concatenate(Y_test)\n",
    "    return X_test, Y_test\n",
    "\n",
    "# just get X_test to make sure it has the right shape\n",
    "X_test, _ = unpack_test()\n",
    "if X_train.shape[-1] != X_test.shape[-1]:\n",
    "    raise ValueError(f'Number of frequency bins in training set spectrograms, {X_train.shape[-1]}, '\n",
    "                     f'does not equal number in test set spectrograms, {X_test.shape[-1]}.')\n",
    "freq_bins = X_test.shape[-1]  # number of columns\n",
    "\n",
    "# concatenate labels into one big string\n",
    "# used for Levenshtein distance + syllable error rate\n",
    "Y_train_labels = [voc.annot.labels.tolist() for voc in train_vds.voc_list]\n",
    "Y_train_labels_for_lev = ''.join([chr(lbl) if type(lbl) is int else lbl\n",
    "                                  for labels in Y_train_labels for lbl in labels])\n",
    "Y_test_labels = [voc.annot.labels.tolist() for voc in test_vds.voc_list]\n",
    "Y_test_labels_for_lev = ''.join([chr(lbl) if type(lbl) is int else lbl\n",
    "                                 for labels in Y_test_labels for lbl in labels])\n",
    "\n",
    "replicates = range(1, num_replicates + 1)\n",
    "\n",
    "NETWORKS = vak.network._load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate labels into one big string\n",
    "# used for Levenshtein distance + syllable error rate\n",
    "Y_train_labels = [voc.annot.labels.tolist() for voc in train_vds.voc_list]\n",
    "Y_train_labels_for_lev = ''.join([chr(lbl) if type(lbl) is int else lbl\n",
    "                                  for labels in Y_train_labels for lbl in labels])\n",
    "Y_test_labels = [voc.annot.labels.tolist() for voc in test_vds.voc_list]\n",
    "Y_test_labels_for_lev = ''.join([chr(lbl) if type(lbl) is int else lbl\n",
    "                                 for labels in Y_test_labels for lbl in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = str(\n",
    "    '/home/nickledave/Documents/repos/tweetynet/src/configs/config_BFSongRepository_gy6or6_.ini'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_config = vak.config.parse_config(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_dur = 60\n",
    "replicate = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_records_path = '/home/nickledave/Documents/data/BFSongRepository/vak/gy6or6/results_190726_153021'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spect_scaler = joblib.load(\n",
    "    os.path.join(training_records_path, 'spect_scaler'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(net_name, net_config) = tuple(a_config.networks.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = unpack_test()\n",
    "# Normalize before reshaping to avoid even more convoluted array reshaping.\n",
    "X_test = spect_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice we don't reshape Y_test\n",
    "(X_test,\n",
    " _,\n",
    " num_batches_test) = vak.utils.data.reshape_data_for_batching(\n",
    "    X_test,\n",
    "    net_config.batch_size,\n",
    "    net_config.time_bins,\n",
    "    Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_config_dict = net_config._asdict()\n",
    "net_config_dict['n_syllables'] = n_classes\n",
    "if 'freq_bins' in net_config_dict:\n",
    "    net_config_dict['freq_bins'] = freq_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dirname_this_net = os.path.join(training_records_path, net_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nickledave/anaconda3/envs/vak-env/lib/python3.6/site-packages/tweetynet/model.py:227: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /home/nickledave/anaconda3/envs/vak-env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/nickledave/anaconda3/envs/vak-env/lib/python3.6/site-packages/tweetynet/model.py:232: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling2d instead.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/nickledave/anaconda3/envs/vak-env/lib/python3.6/site-packages/tweetynet/model.py:260: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/nickledave/anaconda3/envs/vak-env/lib/python3.6/site-packages/tweetynet/model.py:270: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/nickledave/anaconda3/envs/vak-env/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/nickledave/anaconda3/envs/vak-env/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "net = NETWORKS[net_name](**net_config_dict)\n",
    "\n",
    "# we use latest checkpoint when doing summary for learncurve, assume that's \"best trained\"\n",
    "checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir=results_dirname_this_net)\n",
    "\n",
    "meta_file = glob(checkpoint_file + '*meta')\n",
    "if len(meta_file) != 1:\n",
    "    raise ValueError('Incorrect number of meta files for last saved checkpoint.\\n'\n",
    "                     'For checkpoint {}, found these files:\\n'\n",
    "                     '{}'\n",
    "                     .format(checkpoint_file, meta_file))\n",
    "else:\n",
    "    meta_file = meta_file[0]\n",
    "\n",
    "data_file = glob(checkpoint_file + '*data*')\n",
    "if len(data_file) != 1:\n",
    "    raise ValueError('Incorrect number of data files for last saved checkpoint.\\n'\n",
    "                     'For checkpoint {}, found these files:\\n'\n",
    "                     '{}'\n",
    "                     .format(checkpoint_file, data_file))\n",
    "else:\n",
    "    data_file = data_file[0]\n",
    "\n",
    "with tf.Session(graph=net.graph) as sess:\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "    net.restore(sess=sess,\n",
    "                meta_file=meta_file,\n",
    "                data_file=data_file)\n",
    "\n",
    "    for b in range(num_batches_test):  # \"b\" is \"batch number\"\n",
    "        d = {\n",
    "            net.X: X_test[:, b * net_config_dict['time_bins']: (b + 1) * net_config_dict['time_bins'], :],\n",
    "            net.lng: [net_config_dict['time_bins']] * net_config_dict['batch_size']}\n",
    "\n",
    "        if 'Y_pred_test' in locals():\n",
    "            preds = sess.run(net.predict, feed_dict=d)\n",
    "            preds = preds.reshape(net_config_dict['batch_size'], -1)\n",
    "            Y_pred_test = np.concatenate((Y_pred_test, preds),\n",
    "                                         axis=1)\n",
    "        else:\n",
    "            Y_pred_test = sess.run(net.predict, feed_dict=d)\n",
    "            Y_pred_test = Y_pred_test.reshape(net_config_dict['batch_size'], -1)\n",
    "\n",
    "    # again get rid of zero padding predictions\n",
    "    Y_pred_test = Y_pred_test.ravel()[:Y_test.shape[0], np.newaxis]\n",
    "    test_err = np.sum(Y_pred_test != Y_test) / Y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## okay, now look at predictions -- does `vak.test` output match `vak.predict`?\n",
    "We make sure `Y_pred_test` is an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9],\n",
       "       [9],\n",
       "       [9],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_lbl_tb_list = test_vds.lbl_tb_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the lengths of each of the individual labeled timebins vectors for each spectrogram, so we can split `Y_pred_test` up into vectors of the same sizes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_lens = [arr.shape for arr in Y_test_lbl_tb_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we split them up, answer the question we asked above:  \n",
    "how different is output of `lbl_tb2segments` (used by `vak.core.predict`) compared to output of `lbl_tb2label` (used by `vak.core.learncurve.test`)?\n",
    "\n",
    "First of all:  \n",
    "do they return vectors of the same length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_test_seg = vak.utils.labels.lbl_tb2labels(Y_pred_test, train_vds.labelmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12419"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_pred_test_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "timebin_dur = set([voc.metaspect.timebin_dur for voc in train_vds.voc_list])\n",
    "timebin_dur = timebin_dur.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_test_lbl, onsets, offsets = vak.utils.labels.lbl_tb2segments(Y_pred_test,\n",
    "                                                                    train_vds.labelmap,\n",
    "                                                                    timebin_dur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12419,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_test_lbl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, vectors returned by each function are the same length.\n",
    "\n",
    "Okay, what is the edit distance between them?  \n",
    "If 0, it's the same vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_test_lbl_str = ''.join(Y_pred_test_lbl.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vak.metrics.levenshtein(Y_pred_test_seg, Y_pred_test_lbl_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be extra sure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_test_seg == Y_pred_test_lbl_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so that's not the problem -- we're getting the same result for all intents and purposes from `test` and `predict`.\n",
    "\n",
    "## if that's not the problem, what is?\n",
    "\n",
    "So even though error is low, maybe we're not recovering the same segments from `predict` that we have in the test set?\n",
    "\n",
    "To figure that out, we need to go ahead and split up `Y_pred` into labeled timebin vectors of the same size as those in the original test set, segment each vector, and then look at the segments we get out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts = [0]\n",
    "stops = []\n",
    "current_start = 0\n",
    "for a_len in Y_test_lens:\n",
    "    a_len = a_len[0]\n",
    "    stops.append(current_start + a_len)\n",
    "    current_start += a_len\n",
    "    if current_start < Y_test.shape[0]:\n",
    "        starts.append(current_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_lbl_tb_list = []\n",
    "for start, stop in zip(starts, stops):\n",
    "    Y_pred_lbl_tb_list.append(Y_pred_test[start:stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_lens = [arr.shape for arr in Y_pred_lbl_tb_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([pred_len == test_len for pred_len, test_len in zip(Y_pred_lens, Y_test_lens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_labels = []\n",
    "Y_pred_onsets = []\n",
    "Y_pred_offsets = []\n",
    "for a_pred_lbl_tb in Y_pred_lbl_tb_list:\n",
    "    lbl, on, off = vak.utils.labels.lbl_tb2segments(a_pred_lbl_tb, train_vds.labelmap, timebin_dur)\n",
    "    Y_pred_labels.append(lbl)\n",
    "    Y_pred_onsets.append(on)\n",
    "    Y_pred_offsets.append(off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['i', 'i', 'i', 'i', 'i', 'i', 'i', 'b', 'i', 'i', 'i', 'a', 'b',\n",
       "       'c', 'd', 'e', 'e', 'e', 'f', 'g', 'h', 'j', 'k', 'i', 'a', 'b',\n",
       "       'c', 'd', 'e', 'e', 'f', 'g', 'h', 'j', 'k', 'i', 'a', 'b', 'b',\n",
       "       'c', 'd', 'e', 'e', 'f', 'g', 'h', 'j', 'k', 'i', 'a', 'b', 'c',\n",
       "       'd', 'e', 'e', 'f'], dtype='<U1')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_labels[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_labels_from_seg = []\n",
    "Y_test_onsets = []\n",
    "Y_test_offsets = []\n",
    "for a_test_lbl_tb in Y_test_lbl_tb_list:\n",
    "    lbl, on, off = vak.utils.labels.lbl_tb2segments(a_test_lbl_tb, train_vds.labelmap, timebin_dur)\n",
    "    Y_test_labels_from_seg.append(lbl)\n",
    "    Y_test_onsets.append(on)\n",
    "    Y_test_offsets.append(off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['i', 'i', 'i', 'i', 'i', 'i', 'a', 'b', 'c', 'd', 'e', 'e', 'f',\n",
       "       'g', 'h', 'j', 'k', 'i', 'a', 'b', 'c', 'd', 'e', 'e', 'f', 'g',\n",
       "       'h', 'j', 'k', 'i', 'a', 'b', 'c', 'd', 'e', 'e', 'f', 'g', 'h',\n",
       "       'j', 'k', 'i', 'a', 'b', 'c', 'd', 'e', 'e', 'f'], dtype='<U1')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_labels_from_seg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_labels_from_seg[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least for the first vector, there are more segments in the predicted labels.\n",
    "\n",
    "These could be segments that are not in the ground-truth labels because the person annotating the song removed them.\n",
    "\n",
    "As a sanity check, do we recover the ground truth labels if we apply `vak.utils.lbl_tb2segments` to the ground truth label vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(Y_test_labels[0], Y_test_labels_from_seg[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we do.\n",
    "\n",
    "So, yes, we're getting extra segments in our predictions somewhere.\n",
    "\n",
    "How frequent is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_lengths = [Y_pred_seg.shape == Y_test_seg.shape for Y_pred_seg, Y_test_seg in zip(Y_pred_labels, Y_test_labels_from_seg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% with accurate length:  0.0357\n"
     ]
    }
   ],
   "source": [
    "len_acc = sum(same_lengths) / len(same_lengths)\n",
    "print(f'% with accurate length: {len_acc: 0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only about 3% of them are the right lengths\n",
    "\n",
    "So what if we subtract the number of segments in the predicted labels from the number in the ground truth labels?  \n",
    "If the number is negative, there are more segments in the predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_diffs = [Y_test_seg.shape[0] - Y_pred_seg.shape[0] for Y_pred_seg, Y_test_seg in zip(Y_pred_labels, Y_test_labels_from_seg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7, -5, -5, -12, -4]\n"
     ]
    }
   ],
   "source": [
    "print(length_diffs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.666666666666667"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(length_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, there are more segments in the predicted labels.\n",
    "\n",
    "Two approaches to cleaning up:  \n",
    "(1) remove segments lower than a certain duration  \n",
    "  + this might help if all the spurious segments are shorter than typical syllables  \n",
    "  + it won't help though if e.g. calls are being labeled as syllables, and those calls would have been segments in the ground truth data, but the annotator removed those segments since they weren't syllables  \n",
    "  + problem: what label to give the segment to throw away? If silence on both sides (probably almost all cases) could just set to silence?   \n",
    "\n",
    "(2) remove segments based on syntax  \n",
    "  + throw away segments where label is below some threshold of ever occurring  \n",
    "  + this prevents us from doing an analysis where we ask if recovered original syntax, though  \n",
    "  + because of course we cover the original syntax if we use the original syntax to throw away things that don't match it  \n",
    "  + but I think this is a good way to show the work that actually needs to be done to get this to be useful in the lab, and highlights issues with previous work  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nickledave/Documents/data/BFSongRepository/gy6or6/032212\n"
     ]
    }
   ],
   "source": [
    "cd ~/Documents/data/BFSongRepository/gy6or6/032212/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "notmats = glob('*.not.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "notmat0 = loadmat(notmats[0], squeeze_me=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dur = notmat0['min_dur']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually inspecting onsets from first song in test set to compare with predicted onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.936, 1.1  , 1.292, 1.462, 1.636, 1.814, 1.972, 2.064, 2.166,\n",
       "       2.236, 2.31 , 2.382, 2.458, 2.584, 2.704, 2.762, 2.834, 3.054,\n",
       "       3.206, 3.3  , 3.408, 3.48 , 3.554, 3.628, 3.698, 3.83 , 3.968,\n",
       "       4.022, 4.098, 4.322, 4.492, 4.584, 4.696, 4.772, 4.844, 4.92 ,\n",
       "       4.994, 5.128, 5.27 , 5.324, 5.404, 5.624, 5.796, 5.892, 5.996,\n",
       "       6.074, 6.148, 6.222, 6.298])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_onsets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.   , 0.348, 0.392, 0.924, 1.1  , 1.294, 1.462, 1.466, 1.468,\n",
       "       1.636, 1.814, 1.97 , 2.064, 2.166, 2.236, 2.308, 2.382, 2.458,\n",
       "       2.46 , 2.584, 2.704, 2.762, 2.836, 3.054, 3.208, 3.3  , 3.406,\n",
       "       3.48 , 3.554, 3.628, 3.698, 3.83 , 3.968, 4.024, 4.096, 4.324,\n",
       "       4.492, 4.584, 4.624, 4.696, 4.772, 4.844, 4.92 , 4.992, 5.128,\n",
       "       5.268, 5.324, 5.402, 5.624, 5.796, 5.89 , 5.998, 6.074, 6.148,\n",
       "       6.222, 6.298])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_onsets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay there's a couple extra predicted onsets.\n",
    "\n",
    "How many of them are less than the minimum duration for syllables we used when segmenting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 56.  78.  88. 108.  92.  84.  40.  36.  52.  62.  62.  58.  80.  42.\n",
      "  42.  66.  88.  86.  40.  34.  54.  64.  62.  54.  80.  38.  36.  68.\n",
      "  92.  90.  38.  34.  58.  62.  62.  56.  74.  42.  40.  68.  86.  88.\n",
      "  38.  38.  60.  62.  64.  58.  78.]\n",
      "number less than minimum syllable duration used to segment:  0\n"
     ]
    }
   ],
   "source": [
    "durs_test_0 = (Y_test_offsets[0] - Y_test_onsets[0]) * 1000\n",
    "print(durs_test_0)\n",
    "print(\"number of segments with duration less than minimum syllable duration used to segment: \", np.sum(durs_test_0 < min_dur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 36.   2.  38.  76.  78.  84.   2.   0. 102.  90.  84.  40.  38.  52.\n",
      "  60.  62.  60.   0.  78.  42.  40.  68.  86.  90.  36.  32.  58.  62.\n",
      "  62.  52.  78.  40.  36.  66.  94.  90.  38.  34.   6.  58.  64.  60.\n",
      "  54.  78.  42.  40.  68.  88.  90.  38.  40.  58.  64.  62.  58.  80.]\n",
      "number of segments with duration less than minimum syllable duration used to segment:  5\n"
     ]
    }
   ],
   "source": [
    "durs_pred_0 = (Y_pred_offsets[0] - Y_pred_onsets[0]) * 1000\n",
    "print(durs_pred_0)\n",
    "print(\"number of segments with duration less than minimum syllable duration used to segment: \", np.sum(durs_pred_0 < min_dur))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than a couple in the predicted onsets array.\n",
    "What about across *all* the predicted onsets arrays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "durs_pred = []\n",
    "lt_min_dur = []\n",
    "for off, on in zip(Y_pred_offsets, Y_pred_onsets):\n",
    "    durs = (off - on) * 1000\n",
    "    durs_pred.append(durs)\n",
    "    lt_min_dur.append(np.sum(durs < min_dur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 3, 5, 7, 2, 5, 1, 4, 3, 1, 9, 8, 1, 2, 31, 4, 2, 2, 9, 6, 5, 24, 5, 8, 4, 3, 6, 2, 4, 2, 3, 0, 2, 7, 8, 9, 8, 1, 5, 5, 8, 8, 9, 1, 2, 4, 0, 5, 6, 1, 1, 11, 3, 2, 3, 3, 9, 8, 3, 2, 18, 3, 4, 2, 10, 24, 3, 7, 0, 7, 6, 4, 3, 8, 2, 7, 1, 5, 7, 5, 1, 9, 2, 3, 11, 10, 0, 2, 22, 7, 62, 8, 5, 1, 10, 6, 3, 13, 11, 3, 14, 14, 3, 4, 30, 5, 7, 1, 2, 8, 6, 4, 5, 19, 4, 8, 2, 3, 9, 2, 2, 7, 3, 16, 1, 89, 5, 4, 5, 10, 4, 15, 3, 5, 8, 0, 2, 3, 3, 4, 8, 5, 3, 7, 5, 3, 2, 6, 2, 1, 6, 6, 5, 2, 5, 3, 1, 9, 7, 5, 6, 2, 7, 6, 10, 6, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "print(lt_min_dur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay and how does that compare to the number of extra segments in each predicted labels array (regardless of whether the segments are less than the minimum duration)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_extra = []\n",
    "for Y_pred_seg, Y_test_seg in zip(Y_pred_labels, Y_test_labels_from_seg):\n",
    "    num_extra.append(Y_pred_seg.shape[0]-Y_test_seg.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 5, 5, 12, 4, 5, 1, 4, 3, 4, 10, 10, 1, 3, 36, 5, 2, 2, 9, 7, 6, 32, 7, 10, 4, 3, 8, 2, 5, 2, 3, 0, 2, 12, 14, 10, 9, 1, 6, 5, 7, 9, 9, 1, 3, 4, 0, 5, 10, 1, 2, 11, 4, 2, 4, 3, 13, 8, 3, 2, 20, 4, 5, 2, 11, 39, 3, 7, 0, 8, 7, 4, 3, 9, 2, 7, 1, 10, 8, 5, 1, 9, 2, 3, 14, 11, 0, 2, 23, 7, 86, 9, 5, 1, 12, 6, 3, 14, 12, 3, 19, 16, 3, 4, 44, 6, 7, 1, 2, 9, 7, 5, 6, 20, 5, 8, 2, 3, 9, 4, 3, 8, 3, 19, 0, 102, 5, 4, 6, 10, 4, 16, 3, 5, 8, 0, 2, 3, 4, 4, 8, 5, 3, 7, 5, 4, 2, 6, 2, 1, 6, 6, 6, 2, 6, 3, 1, 9, 8, 5, 8, 3, 8, 8, 10, 8, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "print(num_extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, looks similar.\n",
    "\n",
    "So what if we filtered out all the segments less than the minimum duration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_extra_minus_num_lt_min = [extra - lt_dur for extra, lt_dur in zip(num_extra, lt_min_dur)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 0, 5, 2, 0, 0, 0, 0, 3, 1, 2, 0, 1, 5, 1, 0, 0, 0, 1, 1, 8, 2, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 5, 6, 1, 1, 0, 1, 0, -1, 1, 0, 0, 1, 0, 0, 0, 4, 0, 1, 0, 1, 0, 1, 0, 4, 0, 0, 0, 2, 1, 1, 0, 1, 15, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 5, 1, 0, 0, 0, 0, 0, 3, 1, 0, 0, 1, 0, 24, 1, 0, 0, 2, 0, 0, 1, 1, 0, 5, 2, 0, 0, 14, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 2, 1, 1, 0, 3, -1, 13, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 2, 1, 1, 2, 0, 2, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(num_extra_minus_num_lt_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1488095238095237"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(num_extra_minus_num_lt_min).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we'd do a lot better overall, although in a couple cases we get less than the number of syllables in the test set (?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
