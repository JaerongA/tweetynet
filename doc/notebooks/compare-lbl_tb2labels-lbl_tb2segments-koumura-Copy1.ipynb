{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "from glob import glob\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "import vak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VDS_PATH = Path(\n",
    "    '/home/art/Documents/data/birdsong/BirdsongRecognition/vak/Bird6/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vds_path = list(VDS_PATH.glob('*train.vds.json'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  7.4s\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "train_vds = vak.Dataset.load(json_fname=train_vds_path)\n",
    "\n",
    "if train_vds.are_spects_loaded() is False:\n",
    "    train_vds = train_vds.load_spects()\n",
    "\n",
    "X_train = train_vds.spects_list()\n",
    "X_train = np.concatenate(X_train, axis=1)\n",
    "Y_train = train_vds.lbl_tb_list()\n",
    "Y_train = np.concatenate(Y_train)\n",
    "# transpose so rows are time bins\n",
    "X_train = X_train.T\n",
    "\n",
    "n_classes = len(train_vds.labelmap)\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vds_path = list(VDS_PATH.glob('*test.vds.json'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_replicates = 4\n",
    "train_set_durs = [60, 120, 480]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  3.6s\n"
     ]
    }
   ],
   "source": [
    "test_vds = vak.Dataset.load(json_fname=test_vds_path)\n",
    "\n",
    "if test_vds.are_spects_loaded() is False:\n",
    "    test_vds = test_vds.load_spects()\n",
    "\n",
    "if test_vds.labelmap != train_vds.labelmap:\n",
    "    raise ValueError(\n",
    "        f'labelmap of test set, {test_vds.labelmap}, does not match labelmap of training set, '\n",
    "        f'{train_vds.labelmap}'\n",
    "    )\n",
    "\n",
    "def unpack_test():\n",
    "    \"\"\"helper function because we want to get back test set unmodified every time we go through\n",
    "    main loop below, without copying giant arrays\"\"\"\n",
    "    X_test = test_vds.spects_list()\n",
    "    X_test = np.concatenate(X_test, axis=1)\n",
    "    # transpose so rows are time bins\n",
    "    X_test = X_test.T\n",
    "    Y_test = test_vds.lbl_tb_list()\n",
    "    Y_test = np.concatenate(Y_test)\n",
    "    return X_test, Y_test\n",
    "\n",
    "# just get X_test to make sure it has the right shape\n",
    "X_test, _ = unpack_test()\n",
    "if X_train.shape[-1] != X_test.shape[-1]:\n",
    "    raise ValueError(f'Number of frequency bins in training set spectrograms, {X_train.shape[-1]}, '\n",
    "                     f'does not equal number in test set spectrograms, {X_test.shape[-1]}.')\n",
    "freq_bins = X_test.shape[-1]  # number of columns\n",
    "\n",
    "# concatenate labels into one big string\n",
    "# used for Levenshtein distance + syllable error rate\n",
    "Y_train_labels = [voc.annot.labels.tolist() for voc in train_vds.voc_list]\n",
    "Y_train_labels_for_lev = ''.join([chr(lbl) if type(lbl) is int else lbl\n",
    "                                  for labels in Y_train_labels for lbl in labels])\n",
    "Y_test_labels = [voc.annot.labels.tolist() for voc in test_vds.voc_list]\n",
    "Y_test_labels_for_lev = ''.join([chr(lbl) if type(lbl) is int else lbl\n",
    "                                 for labels in Y_test_labels for lbl in labels])\n",
    "\n",
    "replicates = range(1, num_replicates + 1)\n",
    "\n",
    "NETWORKS = vak.network._load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate labels into one big string\n",
    "# used for Levenshtein distance + syllable error rate\n",
    "Y_train_labels = [voc.annot.labels.tolist() for voc in train_vds.voc_list]\n",
    "Y_train_labels_for_lev = ''.join([chr(lbl) if type(lbl) is int else lbl\n",
    "                                  for labels in Y_train_labels for lbl in labels])\n",
    "Y_test_labels = [voc.annot.labels.tolist() for voc in test_vds.voc_list]\n",
    "Y_test_labels_for_lev = ''.join([chr(lbl) if type(lbl) is int else lbl\n",
    "                                 for labels in Y_test_labels for lbl in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = str(\n",
    "    '/home/art/Documents/data/birdsong/BirdsongRecognition/vak/Bird6/'\n",
    "    'learning_curve.190621_123313/'\n",
    "    'config_BirdsongRecognition_bird06_copy.ini'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_config = vak.config.parse_config(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dirname = str(\n",
    "    '/home/art/Documents/data/birdsong/BirdsongRecognition/vak/Bird6/'\n",
    "    'learning_curve.190621_123313'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_dur = 60\n",
    "replicate = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_records_dir = ('records_for_training_set_with_duration_of_'\n",
    "                        + str(train_set_dur) + '_sec_replicate_'\n",
    "                        + str(replicate))\n",
    "training_records_path = os.path.join(results_dirname,\n",
    "                                     'train',\n",
    "                                     training_records_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_name = ('spect_scaler_duration_{}_replicate_{}'\n",
    "               .format(train_set_dur, replicate))\n",
    "spect_scaler = joblib.load(\n",
    "    os.path.join(training_records_path, scaler_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "(net_name, net_config) = tuple(a_config.networks.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = unpack_test()\n",
    "# Normalize before reshaping to avoid even more convoluted array reshaping.\n",
    "X_test = spect_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice we don't reshape Y_test\n",
    "(X_test,\n",
    " _,\n",
    " num_batches_test) = vak.utils.data.reshape_data_for_batching(\n",
    "    X_test,\n",
    "    net_config.batch_size,\n",
    "    net_config.time_bins,\n",
    "    Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_config_dict = net_config._asdict()\n",
    "net_config_dict['n_syllables'] = n_classes\n",
    "if 'freq_bins' in net_config_dict:\n",
    "    net_config_dict['freq_bins'] = freq_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dirname_this_net = os.path.join(training_records_path, net_name)\n",
    "checkpoint_path = os.path.join(results_dirname_this_net, 'checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/art/Documents/repos/coding/birdsong/tweetynet/src/tweetynet/model.py:227: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /home/art/anaconda3/envs/tweetynet/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/art/Documents/repos/coding/birdsong/tweetynet/src/tweetynet/model.py:232: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling2d instead.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/art/Documents/repos/coding/birdsong/tweetynet/src/tweetynet/model.py:260: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/art/Documents/repos/coding/birdsong/tweetynet/src/tweetynet/model.py:270: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/art/anaconda3/envs/tweetynet/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/art/anaconda3/envs/tweetynet/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "net = NETWORKS[net_name](**net_config_dict)\n",
    "\n",
    "# we use latest checkpoint when doing summary for learncurve, assume that's \"best trained\"\n",
    "checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir=checkpoint_path)\n",
    "\n",
    "meta_file = glob(checkpoint_file + '*meta')\n",
    "if len(meta_file) != 1:\n",
    "    raise ValueError('Incorrect number of meta files for last saved checkpoint.\\n'\n",
    "                     'For checkpoint {}, found these files:\\n'\n",
    "                     '{}'\n",
    "                     .format(checkpoint_file, meta_file))\n",
    "else:\n",
    "    meta_file = meta_file[0]\n",
    "\n",
    "data_file = glob(checkpoint_file + '*data*')\n",
    "if len(data_file) != 1:\n",
    "    raise ValueError('Incorrect number of data files for last saved checkpoint.\\n'\n",
    "                     'For checkpoint {}, found these files:\\n'\n",
    "                     '{}'\n",
    "                     .format(checkpoint_file, data_file))\n",
    "else:\n",
    "    data_file = data_file[0]\n",
    "\n",
    "with tf.Session(graph=net.graph) as sess:\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "    net.restore(sess=sess,\n",
    "                meta_file=meta_file,\n",
    "                data_file=data_file)\n",
    "\n",
    "    for b in range(num_batches_test):  # \"b\" is \"batch number\"\n",
    "        d = {\n",
    "            net.X: X_test[:, b * net_config_dict['time_bins']: (b + 1) * net_config_dict['time_bins'], :],\n",
    "            net.lng: [net_config_dict['time_bins']] * net_config_dict['batch_size']}\n",
    "\n",
    "        if 'Y_pred_test' in locals():\n",
    "            preds = sess.run(net.predict, feed_dict=d)\n",
    "            preds = preds.reshape(net_config_dict['batch_size'], -1)\n",
    "            Y_pred_test = np.concatenate((Y_pred_test, preds),\n",
    "                                         axis=1)\n",
    "        else:\n",
    "            Y_pred_test = sess.run(net.predict, feed_dict=d)\n",
    "            Y_pred_test = Y_pred_test.reshape(net_config_dict['batch_size'], -1)\n",
    "\n",
    "    # again get rid of zero padding predictions\n",
    "    Y_pred_test = Y_pred_test.ravel()[:Y_test.shape[0], np.newaxis]\n",
    "    test_err = np.sum(Y_pred_test != Y_test) / Y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_lbl_tb_list = test_vds.lbl_tb_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_lens = [arr.shape for arr in Y_test_lbl_tb_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How different is output of `lbl_tb2segments` (used by `vak.core.predict`) compared to output of `lbl_tb2label` (used by `vak.core.learncurve.test`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_test_seg = vak.utils.labels.lbl_tb2labels(Y_pred_test, train_vds.labelmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3172"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_pred_test_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "timebin_dur = set([voc.metaspect.timebin_dur for voc in train_vds.voc_list])\n",
    "timebin_dur = timebin_dur.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_test_lbl, onsets, offsets = vak.utils.labels.lbl_tb2segments(Y_pred_test,\n",
    "                                                                    train_vds.labelmap,\n",
    "                                                                    timebin_dur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3172,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_test_lbl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_test_lbl_str = ''.join(Y_pred_test_lbl.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vak.metrics.levenshtein(Y_pred_test_seg, Y_pred_test_lbl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
